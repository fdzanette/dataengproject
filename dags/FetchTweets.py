from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from time import time
import json
import tweepy
from datetime import datetime
from pymongo import MongoClient


default_args = {
    'owner': 'Fabricio',
    'depends_on_past': False,
    'start_date': datetime(2022, 1, 12, 00),
    'retries': 1,
    'retry_delay': timedelta(minutes=1)    
}


def collectTweets():

    data_today = datetime.now().strftime("%y-%m-%d-%H-%M-%S")
    out = open(f"collected_tweets_{data_today}.txt", "w")
    print(out.name)
    class MyListener(tweepy.StreamListener):

        def __init__(self, time_limit=60):
            print("Init")
            self.start_time = time()
            self.limit = time_limit
            super(tweepy.StreamListener, self).__init__()
            print("EndInit")
        
        def on_data(self, data):
            
            if (time() - self.start_time) < self.limit:
                itemString = json.dumps(data)
                out.write(itemString + "\n")
            
                return True            
            else:
                print("Finish")
                return False

        def on_error(self, status):
            print(status)
    
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)
    print("Authenticated")
    stream = tweepy.Stream(auth, listener=MyListener(time_limit=20))
    print("Streaming")
    stream.filter(track=["Djokovic", "Djokovid", "Novax Djocovid"])

    ## Save tweets in mongodb
    CONNECTION_STRING = "mongodb://mongo:mongo@172.20.0.4:27017/" ## IP generated by docker network
    client = MongoClient(CONNECTION_STRING)
    db = client['tweets']
    tweets_collection = db['fetched_tweets']    
    with open(out.name, 'r') as f:
        file_data = f.readlines()
    parsed_tweets = [json.loads( json.loads(i) ) for i in file_data]
    tweets_collection.insert_many(parsed_tweets)
    client.close()  

with DAG(dag_id='FetchTweets', schedule_interval="*/15 * * * *", default_args=default_args, catchup=False) as dag:

    StartProcess = BashOperator(
    task_id='Start_process',
    bash_command='echo "Start Processing!"'
    
)

    StreamTweets = PythonOperator(
        task_id='Stream_tweets',
        python_callable = collectTweets,
        
)

    FinishProcess = BashOperator(
    task_id='Process_finished',
    bash_command='echo "Dag finished successfully!"'
    
)

StartProcess >> StreamTweets >> FinishProcess